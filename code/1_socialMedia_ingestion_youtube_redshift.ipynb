{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9049a04-7aeb-470e-b9b7-7c8a8ac3ac75",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38639118-4548-41e4-8289-3e311f2b8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import yxdb\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2267c-ba2d-443a-a98f-10d3772486d6",
   "metadata": {},
   "source": [
    "## import helper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5575b7-60b4-4a6a-866b-0c0be68a197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_GAM2025 import gam_info\n",
    "\n",
    "from functions import execute_sql_query\n",
    "import test_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc37984-f240-4046-a8ec-8f65dbf0d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GAM2025_Lookup.xlsx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gam_info['lookup_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec311860-8be4-4a7f-a695-b7f6a97570a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Status</th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>Channel Name</th>\n",
       "      <th>Service</th>\n",
       "      <th>ServiceID</th>\n",
       "      <th>Channel Group</th>\n",
       "      <th>Channel URL</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>Linked FB Account</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Youtube</td>\n",
       "      <td>active</td>\n",
       "      <td>UCSNvArbpIG1Kp6EGpx-QeZg</td>\n",
       "      <td>EastEnders</td>\n",
       "      <td>Studios</td>\n",
       "      <td>WOR</td>\n",
       "      <td>BBC Studios</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAM2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Platform  Status                Channel ID Channel Name  Service  \\\n",
       "518  Youtube  active  UCSNvArbpIG1Kp6EGpx-QeZg   EastEnders  Studios   \n",
       "\n",
       "    ServiceID Channel Group Channel URL Channel Username Linked FB Account  \\\n",
       "518       WOR   BBC Studios         NaN              NaN               NaN   \n",
       "\n",
       "        Year  \n",
       "518  GAM2025  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# country\n",
    "country_codes = pd.read_excel(f\"../../{gam_info['lookup_file']}\", sheet_name='CountryID')\n",
    "\n",
    "# week \n",
    "week_tester = pd.read_excel(f\"../../{gam_info['lookup_file']}\", sheet_name='GAM Period')\n",
    "week_tester['w/c'] = pd.to_datetime(week_tester['w/c'])\n",
    "week_tester['week_ending'] = pd.to_datetime(week_tester['week_ending'])\n",
    "\n",
    "#Â social media accounts\n",
    "dtype_dict = {'Channel ID': 'str',\n",
    "              'Linked FB Account': 'str'}\n",
    "socialmedia_accounts = pd.read_excel(f\"../../{gam_info['lookup_file']}\", dtype=dtype_dict,\n",
    "                                     sheet_name='Social Media Accounts new')\n",
    "\n",
    "socialmedia_accounts = socialmedia_accounts[(socialmedia_accounts['Platform'] == 'Youtube')\n",
    "                                            & \n",
    "                                            (socialmedia_accounts['Status'] == 'active')]\n",
    "socialmedia_accounts = socialmedia_accounts.rename(columns={'Excluding UK': 'Channel Group'})\n",
    "\n",
    "channel_ids = socialmedia_accounts['Channel ID'].unique().tolist()\n",
    "formatted_channel_ids = ', '.join(f\"'{channel_id}'\" for channel_id in channel_ids)\n",
    "socialmedia_accounts.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172943e-5892-4d39-8b73-85f932436bcc",
   "metadata": {},
   "source": [
    "# automated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995f0ac1-4d8a-43b9-a844-78e10f12bd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...testing Channel ID...\n",
      "Fail - not all elements were retrieved\n",
      "...updating logbook...\n",
      "\n",
      "All weeks are present in the dataset.\n",
      "...updating logbook...\n",
      "\n",
      "Missing weeks for each group:\n",
      "     Week Number  YearGAE        w/c week_ending                Channel ID\n",
      "0             14     2025 2024-04-01  2024-04-07  UCCiLIKu4E0FHkIGR0bqCPiw\n",
      "1             36     2025 2024-09-02  2024-09-08  UCCiLIKu4E0FHkIGR0bqCPiw\n",
      "2             49     2025 2024-12-02  2024-12-08  UCCiLIKu4E0FHkIGR0bqCPiw\n",
      "3              6     2025 2025-02-03  2025-02-09  UCCiLIKu4E0FHkIGR0bqCPiw\n",
      "4              9     2025 2025-02-24  2025-03-02  UCCiLIKu4E0FHkIGR0bqCPiw\n",
      "..           ...      ...        ...         ...                       ...\n",
      "221            9     2025 2025-02-24  2025-03-02  UCtRf5L5EPwrzFpMWS69Nelw\n",
      "222           10     2025 2025-03-03  2025-03-09  UCtRf5L5EPwrzFpMWS69Nelw\n",
      "223           11     2025 2025-03-10  2025-03-16  UCtRf5L5EPwrzFpMWS69Nelw\n",
      "224           12     2025 2025-03-17  2025-03-23  UCtRf5L5EPwrzFpMWS69Nelw\n",
      "225           13     2025 2025-03-24  2025-03-30  UCtRf5L5EPwrzFpMWS69Nelw\n",
      "\n",
      "[226 rows x 5 columns]\n",
      "...updating logbook...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = f\"\"\" \n",
    "    SELECT \n",
    "        yt_channel_id, \n",
    "        yt_subscribed_status, \n",
    "        yt_country_code,\n",
    "        yt_metric_id, \n",
    "        yt_metric_period, \n",
    "        yt_metric_end_time, \n",
    "        yt_metric_value \n",
    "    FROM \n",
    "        redshiftdb.central_insights.yt_channel_insights \n",
    "    WHERE\n",
    "        yt_metric_id = 'views' \n",
    "        AND\n",
    "        yt_channel_id IN ({formatted_channel_ids})\n",
    "        AND \n",
    "        yt_metric_end_time BETWEEN '{gam_info['weekEnding_start']}' AND '{gam_info['weekEnding_end']}'\n",
    "    ;\n",
    "    \"\"\"\n",
    "'''\n",
    "df = execute_sql_query(sql_query)\n",
    "    \n",
    "df.to_csv(f\"../data/raw/Youtube/{gam_info['file_timeinfo']}_YT_geography_redshift_extract.csv\", \n",
    "            index=False)\n",
    "'''\n",
    "yt_views = pd.read_csv(f\"../data/raw/Youtube/{gam_info['file_timeinfo']}_YT_geography_redshift_extract.csv\")\n",
    "\n",
    "renaming = {'yt_channel_id': 'Channel ID',\n",
    "            'yt_country_code': 'YouTube Codes',\n",
    "            'yt_metric_end_time': 'week_ending',\n",
    "            }\n",
    "yt_views.rename(columns = renaming, inplace=True)\n",
    "yt_views['week_ending'] = pd.to_datetime(yt_views['week_ending'])\n",
    "################################### Testing ################################### \n",
    "test_step = 'testing yt_channel_insights return from redshift'\n",
    "\n",
    "column_name = 'Channel ID'\n",
    "test_functions.test_filter_elements_returned(yt_views, channel_ids, column_name, \"1_YT_6\")\n",
    "\n",
    "test_functions.test_weeks_presence('week_ending', yt_views, week_tester, '1_YT_7', test_step)\n",
    "\n",
    "test_functions.test_weeks_presence_per_account('week_ending', column_name, yt_views, week_tester, '1_YT_8', test_step)\n",
    "\n",
    "################################### Testing ################################### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90723f13-bafb-4095-be79-777369edef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join test 1_YT_9 failed: Issues found.\n",
      "Issues with df_left (rows present in df_left but not in df_right)\n",
      "...updating logbook...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['HM'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add PlaceID\n",
    "cols = ['PlaceID', 'YouTube Codes']\n",
    "yt_views_cleanCountry = yt_views.merge(country_codes[cols], on=['YouTube Codes'], how='left', indicator=True)\n",
    "\n",
    "################################### Testing ################################### \n",
    "test_step = 'adding country codes GAM'\n",
    "\n",
    "test_functions.test_inner_join(yt_views, country_codes[cols], ['YouTube Codes'], '1_YT_9', test_step, focus='left')\n",
    "\n",
    "################################### Testing ################################### \n",
    "# TODO add HM to GAM lookup\n",
    "yt_views_cleanCountry[yt_views_cleanCountry._merge == 'left_only']['YouTube Codes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0439af4a-b111-4018-a51c-eb457934f497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>PlaceID</th>\n",
       "      <th>week_ending</th>\n",
       "      <th>view_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550947</th>\n",
       "      <td>UC_QuRsbRQpaC4iS8WlxW0NA</td>\n",
       "      <td>ZAM</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>528.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Channel ID PlaceID week_ending  view_country\n",
       "550947  UC_QuRsbRQpaC4iS8WlxW0NA     ZAM  2025-03-23         528.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join test 1_YT_9 successful: No issues found.\n",
      "...updating logbook...\n",
      "\n",
      "...testing if merge leads to more rows on the metric side\n",
      "pass! :)\n",
      "...updating logbook...\n",
      "\n",
      "...updating logbook...\n",
      "\n",
      "Pass - No larger than 1 values\n",
      "...updating logbook...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df_perCountry = yt_views_cleanCountry.groupby([\n",
    "        'Channel ID',\n",
    "        'PlaceID',#country\n",
    "        'week_ending'\n",
    "    ]).agg({'yt_metric_value': 'sum'}).reset_index()\n",
    "grouped_df_perCountry = grouped_df_perCountry.rename(columns={'yt_metric_value': 'view_country'})\n",
    "display(grouped_df_perCountry.sample())\n",
    "\n",
    "# Group by the specified columns and sum the yt_metric_value\n",
    "grouped_df_allCountries = yt_views_cleanCountry.groupby([\n",
    "    'Channel ID',\n",
    "    'week_ending'\n",
    "]).agg({'yt_metric_value': 'sum'}).reset_index()\n",
    "grouped_df_allCountries = grouped_df_allCountries.rename(columns={'yt_metric_value': 'total_view_country'})\n",
    "#display(grouped_df_allCountries.sample())\n",
    "\n",
    "country_proportion = grouped_df_allCountries.merge(grouped_df_perCountry, \n",
    "                                                   on=['Channel ID', 'week_ending'], \n",
    "                                                   how='inner')\n",
    "country_proportion['country_%'] = (country_proportion['view_country'] / country_proportion['total_view_country'])\n",
    "\n",
    "################################### Testing ################################### \n",
    "# todo: add a test that sums country % and needs to come to a very very very exact 100% (at least 8 decimals)\n",
    "test_step = 'calculating % country'\n",
    "cols= ['Channel ID', 'week_ending']\n",
    "test_functions.test_inner_join(grouped_df_allCountries, grouped_df_perCountry, cols, \"1_YT_9\", test_step)\n",
    "\n",
    "test_functions.test_merge_row_count(country_proportion, grouped_df_perCountry, '1_YT_10', test_step)\n",
    "\n",
    "test_functions.test_percentage(country_proportion,  cols, '1_YT_11', test_step)\n",
    "\n",
    "test_functions.test_larger_val(country_proportion,  'country_%', '1_YT_12', test_step, val=1)\n",
    "\n",
    "################################### Testing ################################### \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0085ec-da61-42f1-abc7-b2e5bfddc881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...testing Channel ID...\n",
      "Fail - not all elements were retrieved\n",
      "...updating logbook...\n",
      "\n",
      "All weeks are present in the dataset.\n",
      "...updating logbook...\n",
      "\n",
      "All weeks are present in the dataset for each group.\n",
      "...updating logbook...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query= f\"\"\"\n",
    "    SELECT \n",
    "        yt_channel_id, \n",
    "        week_ending\n",
    "    FROM \n",
    "        redshiftdb.central_insights.yt_channel_metadata \n",
    "    WHERE \n",
    "        yt_channel_id IN ({formatted_channel_ids})\n",
    "        AND\n",
    "        week_ending BETWEEN '{gam_info['weekEnding_start']}' AND '{gam_info['weekEnding_end']}'\n",
    "        ;\n",
    "\"\"\"\n",
    "'''\n",
    "df = execute_sql_query(sql_query)\n",
    "\n",
    "df.to_csv(f\"../data/raw/Youtube/{gam_info['file_timeinfo']}_YT_geography_redshift_metadata.csv\", index=False)\n",
    "\n",
    "'''\n",
    "metadata = pd.read_csv(f\"../data/raw/Youtube/{gam_info['file_timeinfo']}_YT_geography_redshift_metadata.csv\")\n",
    "\n",
    "\n",
    "renaming = {'yt_channel_id': 'Channel ID', }\n",
    "metadata.rename(columns=renaming, inplace=True)\n",
    "metadata['week_ending'] = pd.to_datetime(metadata['week_ending'])\n",
    "\n",
    "################################### Testing ################################### \n",
    "\n",
    "test_step ='testing metadata return from redshift'\n",
    "\n",
    "test_functions.test_filter_elements_returned(metadata, 'Channel ID', column_name, '1_YT_13', test_step)\n",
    "\n",
    "test_functions.test_weeks_presence('week_ending', metadata, week_tester, '1_YT_14', test_step)\n",
    "\n",
    "test_functions.test_weeks_presence_per_account('week_ending', 'Channel ID', metadata, week_tester, '1_YT_15', test_step)\n",
    "\n",
    "################################### Testing ################################### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff36294-9b67-42e6-a550-78ba3bdfbd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join test 1_YT_16 failed: Issues found.\n",
      "Issues with df_right (rows present in df_right but not in df_left)\n",
      "...updating logbook...\n",
      "\n",
      "...testing if merge leads to more rows on the metric side\n",
      "pass! :)\n",
      "...updating logbook...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "automated_country = country_proportion.merge(metadata, \n",
    "                                            on=['Channel ID', 'week_ending'], \n",
    "                                            how='inner')\n",
    "automated_country = automated_country.merge(week_tester[['w/c', 'week_ending']], \n",
    "                                        on=['week_ending'], \n",
    "                                        how='left')\n",
    "country_cols = ['w/c', 'Channel ID', 'PlaceID', 'country_%', ]\n",
    "automated_country = automated_country[country_cols]\n",
    "################################### Testing ################################### \n",
    "test_step = 'combining country metric and metadata'\n",
    "\n",
    "test_functions.test_inner_join(country_proportion, metadata, ['Channel ID', 'week_ending'], '1_YT_16', test_step)\n",
    "\n",
    "test_functions.test_merge_row_count(country_proportion, automated_country, '1_YT_17', test_step)\n",
    "################################### Testing ################################### \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b98a1-12f4-40d1-a002-d9b10b082a9a",
   "metadata": {},
   "source": [
    "#Â manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90b7ea-00cc-4889-bc6d-58193d4b7f5d",
   "metadata": {},
   "source": [
    "##Â import media action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f5eb5b-140c-4f71-8b54-459395c9ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_map = pd.read_excel(f\"../../{gam_info['lookup_file']}\", sheet_name='CountryID')[['PlaceID', 'YouTube Codes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c096567-4694-4369-9325-a8817f640c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: review with minnie for individual exports\n",
    "#because it contains geography or should we use table instead?\n",
    "\n",
    "path = f\"../data/raw/Youtube/{gam_info['file_timeinfo']}_manual/\"\n",
    "dataframes = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.xlsx'):  # Assuming the files are excel files\n",
    "        \n",
    "        try:\n",
    "            file_path = os.path.join(path, filename)\n",
    "            df = pd.read_excel(file_path, sheet_name='Chart data')\n",
    "            df['Channel ID'] = filename.split('.')[0].split(' - ')[0]\n",
    "            df['Channel title'] = filename.split('.')[0].split(' - ')[0]\n",
    "            df['source_path'] = path+filename\n",
    "            \n",
    "            dataframes.append(df)\n",
    "        except:\n",
    "            print(filename)\n",
    "media_action_df = pd.concat(dataframes)\n",
    "\n",
    "def get_week_dates(date):\n",
    "    if date.weekday() != 6:  # Check if the date is not a Sunday\n",
    "        raise ValueError(\"The input date must be a Sunday.\")\n",
    "    \n",
    "    from_date = date + pd.Timedelta(days=1)  # Monday after the given Sunday\n",
    "    to_date = from_date + pd.Timedelta(days=6)  # Sunday after the Monday\n",
    "    return from_date, to_date\n",
    "\n",
    "media_action_df['Date'] = pd.to_datetime(media_action_df['Date'])\n",
    "\n",
    "# Apply the function to get FromDate and ToDate\n",
    "media_action_df['w/c'], media_action_df['week_ending'] = zip(*media_action_df['Date'].apply(get_week_dates))\n",
    "media_action_df = media_action_df.rename(columns={'Geography': 'YouTube Codes'})\n",
    "media_action_df = media_action_df.merge(country_map, on='YouTube Codes', how='left')\n",
    "# Group by Geography, FromDate, ToDate, and filename to sum Views\n",
    "media_action_df = media_action_df.groupby(['w/c', 'week_ending', 'Channel ID', 'Channel title', 'PlaceID', 'source_path']).agg({'Views': 'sum'}).reset_index()\n",
    "\n",
    "media_action_df['Channel Group'] = 'BBC Media Action'\n",
    "\n",
    "channel_ids = {'Aksi Kita Indonesia': 'aksikitaindo', }\n",
    "media_action_df['Channel ID'] = media_action_df['Channel ID'].replace(channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5584d0a-9764-4e3c-b827-ac8471aee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ma_global = media_action_df.groupby(['w/c', 'Channel ID'])['Views'].sum().reset_index()\n",
    "ma_country_df = media_action_df.merge(_ma_global, on=['w/c', 'Channel ID'], how='left',\n",
    "                                      suffixes=['_country', '_global'])\n",
    "ma_country_df['country_%'] = ma_country_df['Views_country'] / ma_country_df['Views_global']\n",
    "ma_country_df = ma_country_df[automated_country.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31848384-fd05-4269-9299-c81e4c080acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_country = pd.concat([automated_country, ma_country_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed565078-7e98-43dd-9552-985bcd2841a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w/c</th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>PlaceID</th>\n",
       "      <th>country_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UC0JypFmHP-9wh5A3JjJLpcA</td>\n",
       "      <td>AFG</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UC0JypFmHP-9wh5A3JjJLpcA</td>\n",
       "      <td>ALB</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UC0JypFmHP-9wh5A3JjJLpcA</td>\n",
       "      <td>ALG</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UC0JypFmHP-9wh5A3JjJLpcA</td>\n",
       "      <td>ANG</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UC0JypFmHP-9wh5A3JjJLpcA</td>\n",
       "      <td>ARG</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w/c                Channel ID PlaceID  country_%\n",
       "0 2024-04-01  UC0JypFmHP-9wh5A3JjJLpcA     AFG   0.000007\n",
       "1 2024-04-01  UC0JypFmHP-9wh5A3JjJLpcA     ALB   0.000007\n",
       "2 2024-04-01  UC0JypFmHP-9wh5A3JjJLpcA     ALG   0.000014\n",
       "3 2024-04-01  UC0JypFmHP-9wh5A3JjJLpcA     ANG   0.000001\n",
       "4 2024-04-01  UC0JypFmHP-9wh5A3JjJLpcA     ARG   0.000004"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8beca-59cc-46e3-a6e2-497e3bf0b3d9",
   "metadata": {},
   "source": [
    "## import Serbian & Sinhala Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf2c752-018b-4685-bd77-5c3eab386c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['w/c', 'Channel ID', 'PlaceID', 'country_%'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser_sin_df = pd.read_excel(\"../data/raw/Youtube/serbian sinhala youtube.xlsx\", \n",
    "                           sheet_name='SERSIN')\n",
    "ser_sin_df.rename(columns={'Geography': 'YouTube Codes',\n",
    "                           'Channel': 'Channel ID',\n",
    "                           'Total': 'country_%'}, inplace=True)\n",
    "#Â join country codes \n",
    "ser_sin_df = ser_sin_df.merge(country_codes[['YouTube Codes', 'PlaceID']], on=['YouTube Codes'], indicator=True, how='left')\n",
    "ser_sin_df = ser_sin_df[country_cols]\n",
    "\n",
    "ser_sin_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ce2b83f-4a28-4de3-8049-067213a193a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows in additional_df that are NOT in master_df\n",
    "additional_rows = ser_sin_df[~ser_sin_df.apply(tuple, axis=1).isin(youtube_country.apply(tuple, axis=1))]\n",
    "\n",
    "# Append new rows to master_df\n",
    "youtube_country_2 = pd.concat([youtube_country, additional_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1030b561-a412-4a20-b1ef-bc8cbd77eba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w/c</th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>PlaceID</th>\n",
       "      <th>country_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UCCrAKchnDFMhrKeXSYWXjGA</td>\n",
       "      <td>UAE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UCCrAKchnDFMhrKeXSYWXjGA</td>\n",
       "      <td>ALB</td>\n",
       "      <td>0.000799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UCCrAKchnDFMhrKeXSYWXjGA</td>\n",
       "      <td>ARG</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UCCrAKchnDFMhrKeXSYWXjGA</td>\n",
       "      <td>OST</td>\n",
       "      <td>0.047364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>UCCrAKchnDFMhrKeXSYWXjGA</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0.017587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w/c                Channel ID PlaceID  country_%\n",
       "0 2024-04-01  UCCrAKchnDFMhrKeXSYWXjGA     UAE   0.000000\n",
       "1 2024-04-01  UCCrAKchnDFMhrKeXSYWXjGA     ALB   0.000799\n",
       "2 2024-04-01  UCCrAKchnDFMhrKeXSYWXjGA     ARG   0.000000\n",
       "3 2024-04-01  UCCrAKchnDFMhrKeXSYWXjGA     OST   0.047364\n",
       "4 2024-04-01  UCCrAKchnDFMhrKeXSYWXjGA     AUS   0.017587"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser_sin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563de0f6-d56b-4324-8b2c-75d115b0e4fd",
   "metadata": {},
   "source": [
    "# store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "783c510c-5323-4335-8a22-739284fb17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_country_2.to_csv(f\"../data/processed/Youtube/{gam_info['file_timeinfo']}_country.csv\", \n",
    "                         index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ddf21-40ed-4856-9116-b5401c046952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
